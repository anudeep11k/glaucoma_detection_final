{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b27d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in a notebook cell (prefix with !)\n",
    "!pip install -q tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba93b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x129134e30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "PROJECT_ROOT = \"/Users/anudeep/Documents/glaucoma_detection\"\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"data\", \"clinical_notes.csv\")\n",
    "OUT_BASE = os.path.join(PROJECT_ROOT, \"models\")\n",
    "os.makedirs(OUT_BASE, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0cec7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000 Pos: 5048\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>language</th>\n",
       "      <th>maritalstatus</th>\n",
       "      <th>note</th>\n",
       "      <th>gpt4_summary</th>\n",
       "      <th>glaucoma</th>\n",
       "      <th>use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.56</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "      <td>non-hispanic</td>\n",
       "      <td>english</td>\n",
       "      <td>single</td>\n",
       "      <td>ms. PERSON is a 56 yo woman presenting to esta...</td>\n",
       "      <td>the 56 y/o female patient has optic nerve head...</td>\n",
       "      <td>1</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.91</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>non-hispanic</td>\n",
       "      <td>english</td>\n",
       "      <td>single</td>\n",
       "      <td>referred for evaluation of narrow angles ou #p...</td>\n",
       "      <td>patient was referred for narrow angle evaluati...</td>\n",
       "      <td>1</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.30</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>non-hispanic</td>\n",
       "      <td>english</td>\n",
       "      <td>single</td>\n",
       "      <td>1. left upper lid ptosis: occurred after botox...</td>\n",
       "      <td>patient experienced ptosis, ear and eye pain, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.52</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>non-hispanic</td>\n",
       "      <td>english</td>\n",
       "      <td>single</td>\n",
       "      <td>right plano +0.50 082 left LOCATION -0.50 83 a...</td>\n",
       "      <td>the patient has primary open angle glaucoma - ...</td>\n",
       "      <td>1</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82.52</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "      <td>non-hispanic</td>\n",
       "      <td>english</td>\n",
       "      <td>divorced</td>\n",
       "      <td>in step. os with nonspecific peripheral defect...</td>\n",
       "      <td>the patient has nonspecific peripheral defects...</td>\n",
       "      <td>1</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender   race     ethnicity language maritalstatus  \\\n",
       "0  56.56  female  black  non-hispanic  english        single   \n",
       "1  53.91  female  white  non-hispanic  english        single   \n",
       "2  46.30  female  white  non-hispanic  english        single   \n",
       "3  66.52    male  white  non-hispanic  english        single   \n",
       "4  82.52  female  black  non-hispanic  english      divorced   \n",
       "\n",
       "                                                note  \\\n",
       "0  ms. PERSON is a 56 yo woman presenting to esta...   \n",
       "1  referred for evaluation of narrow angles ou #p...   \n",
       "2  1. left upper lid ptosis: occurred after botox...   \n",
       "3  right plano +0.50 082 left LOCATION -0.50 83 a...   \n",
       "4  in step. os with nonspecific peripheral defect...   \n",
       "\n",
       "                                        gpt4_summary  glaucoma       use  \n",
       "0  the 56 y/o female patient has optic nerve head...         1  training  \n",
       "1  patient was referred for narrow angle evaluati...         1  training  \n",
       "2  patient experienced ptosis, ear and eye pain, ...         0  training  \n",
       "3  the patient has primary open angle glaucoma - ...         1  training  \n",
       "4  the patient has nonspecific peripheral defects...         1  training  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "# minimal checks\n",
    "assert 'gpt4_summary' in df.columns and 'glaucoma' in df.columns\n",
    "\n",
    "# clean summaries inplace\n",
    "def _clean(s): return re.sub(r'\\s+',' ', str(s).strip().lower())\n",
    "df['gpt4_summary'] = df['gpt4_summary'].astype(str).apply(_clean)\n",
    "\n",
    "# map labels yes/no -> 1/0 (assumes these values exist)\n",
    "df['glaucoma'] = df['glaucoma'].map({'yes':1, 'no':0})\n",
    "df = df.dropna(subset=['glaucoma']).reset_index(drop=True)\n",
    "df['glaucoma'] = df['glaucoma'].astype(int)\n",
    "\n",
    "print(\"Rows:\", len(df), \"Pos:\", df['glaucoma'].sum())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "149220a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test: 6999 1501 1500\n",
      "Races in test: race\n",
      "white    1153\n",
      "black     224\n",
      "asian     123\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === FIXED STRATIFIED SPLIT: ensure Asian, Black, White appear in test ===\n",
    "\n",
    "# Create combined stratification key\n",
    "df['strat_key'] = df['glaucoma'].astype(str) + \"_\" + df['race'].astype(str)\n",
    "\n",
    "# First split: train_val vs test\n",
    "train_val, test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.15,\n",
    "    stratify=df['strat_key'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Second split: train vs val\n",
    "train, val = train_test_split(\n",
    "    train_val,\n",
    "    test_size=0.1764706,  # â‰ˆ 0.15 overall val\n",
    "    stratify=train_val['strat_key'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "print(\"Train/Val/Test:\", len(train), len(val), len(test))\n",
    "print(\"Races in test:\", test['race'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e75905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME = nlpie/tiny-biobert\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = \"nlpie/tiny-biobert\"\n",
    "print(\"MODEL_NAME =\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a36bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "MAX_LEN = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14430180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a9cc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpie/tiny-biobert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b7044d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    return tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LEN)\n",
    "\n",
    "train_enc = tokenize(train['gpt4_summary'].tolist())\n",
    "val_enc   = tokenize(val['gpt4_summary'].tolist())\n",
    "test_enc  = tokenize(test['gpt4_summary'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4432f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self): \n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, i):\n",
    "        item = {k: torch.tensor(v[i]) for k,v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[i], dtype=torch.long)  # Changed to long!\n",
    "        return item\n",
    "\n",
    "# Recreate datasets with new label type\n",
    "train_dataset = SimpleDS(train_enc, train['glaucoma'].tolist())\n",
    "val_dataset = SimpleDS(val_enc, val['glaucoma'].tolist())\n",
    "test_dataset = SimpleDS(test_enc, test['glaucoma'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f72352e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 5351, 2786, 11534, 1112, 170, 176, 15554...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 2623, 1884, 6602, 22259, 7589, 1114, 366...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 5465, 194, 119, 184, 119, 2130, 1114, 22...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 5351, 1110, 170, 176, 15554, 8178, 1161,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 2588, 194, 119, 184, 119, 2130, 1114, 16...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [101, 5351, 2786, 11534, 1112, 170, 176, 15554...   \n",
       "1  [101, 2623, 1884, 6602, 22259, 7589, 1114, 366...   \n",
       "2  [101, 5465, 194, 119, 184, 119, 2130, 1114, 22...   \n",
       "3  [101, 5351, 1110, 170, 176, 15554, 8178, 1161,...   \n",
       "4  [101, 2588, 194, 119, 184, 119, 2130, 1114, 16...   \n",
       "\n",
       "                                      attention_mask  labels  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       0  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       0  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       1  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       0  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert train_dataset to dataframe for inspection\n",
    "inspection_df = pd.DataFrame({\n",
    "    'input_ids': [item['input_ids'].tolist() for item in train_dataset],\n",
    "    'attention_mask': [item['attention_mask'].tolist() for item in train_dataset],\n",
    "    'labels': [item['labels'].item() for item in train_dataset]\n",
    "})\n",
    "inspection_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c01e1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    logits = pred.predictions\n",
    "\n",
    "    # convert logits to a single prob-per-sample\n",
    "    if getattr(logits, \"ndim\", None) == 2 and logits.shape[1] == 2:\n",
    "        probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "    else:\n",
    "        probs = torch.sigmoid(torch.tensor(np.asarray(logits).reshape(-1))).numpy()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1, \"auc\": auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "513385b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TAG = MODEL_NAME.split(\"/\")[-1].replace(\".\", \"_\")\n",
    "OUT_DIR_MODEL = os.path.join(OUT_BASE, f\"transformer_{MODEL_TAG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "028cdf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: [0 1] class_weights: [1.00966532 0.99051797]\n",
      "WeightedTrainer ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/7hr355c52nj926nqh954n2ym0000gn/T/ipykernel_57391/1980626966.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Compute class weights and use WeightedTrainer\n",
    "# -------------------------\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np, torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding as data_collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "# Compute weights\n",
    "classes = np.unique(train['glaucoma'])\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train['glaucoma'].values)\n",
    "weight_tensor = torch.tensor(class_weights, dtype=torch.float).to(model.device)\n",
    "print(\"classes:\", classes, \"class_weights:\", class_weights)\n",
    "\n",
    "# Safe WeightedTrainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# TrainingArgs (compatible)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR_MODEL,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    save_total_limit=2,\n",
    "    seed=SEED,\n",
    "    logging_dir=os.path.join(OUT_DIR_MODEL, \"logs\"),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Instantiate WeightedTrainer\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"WeightedTrainer ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fb2a4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset type: <class '__main__.SimpleDS'>\n",
      "is torch Dataset: True\n",
      "data_collator type: <class 'transformers.data.data_collator.DataCollatorWithPadding'>\n",
      "data_collator callable?: True\n",
      "train_dataset[0] type: <class 'dict'>\n",
      "keys: ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"train_dataset type:\", type(train_dataset))\n",
    "print(\"is torch Dataset:\", isinstance(train_dataset, torch.utils.data.Dataset))\n",
    "\n",
    "print(\"data_collator type:\", type(data_collator))\n",
    "print(\"data_collator callable?:\", callable(data_collator))\n",
    "\n",
    "# show a single item from train_dataset\n",
    "try:\n",
    "    item0 = train_dataset[0]\n",
    "    print(\"train_dataset[0] type:\", type(item0))\n",
    "    if isinstance(item0, dict):\n",
    "        print(\"keys:\", list(item0.keys()))\n",
    "    else:\n",
    "        print(repr(item0)[:500])\n",
    "except Exception as e:\n",
    "    print(\"Error reading train_dataset[0]:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa10e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'> {'input_ids': tensor([[  101,  5351,  2786,  ...,     0,     0,     0],\n",
      "        [  101,  2623,  1884,  ...,     0,     0,     0],\n",
      "        [  101,  5465,   194,  ...,     0,     0,     0],\n",
      "        ...,\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(train_dataset, batch_size=training_args.per_device_train_batch_size, collate_fn=data_collator)\n",
    "batch = next(iter(dl))\n",
    "print(type(batch), list(batch.keys()) if isinstance(batch, dict) else repr(batch)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0045f42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anudeep/miniconda3/envs/tf_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='5250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5250/5250 04:22, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.574900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.529900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.511800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.482400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.376200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.356900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anudeep/miniconda3/envs/tf_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/anudeep/miniconda3/envs/tf_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/anudeep/miniconda3/envs/tf_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/anudeep/miniconda3/envs/tf_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/anudeep/miniconda3/envs/tf_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5250, training_loss=0.46808145286923364, metrics={'train_runtime': 263.62, 'train_samples_per_second': 159.298, 'train_steps_per_second': 19.915, 'total_flos': 150537951857664.0, 'train_loss': 0.46808145286923364, 'epoch': 6.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27dd1793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anudeep/miniconda3/envs/tf_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8156\n",
      "Accuracy: 0.7347\n",
      "Precision: 0.7442\n",
      "Recall/Sensitivity: 0.7226\n",
      "F1: 0.7332\n",
      "Sensitivity: 0.7226\n",
      "Specificity: 0.7470\n"
     ]
    }
   ],
   "source": [
    "pred_out = trainer.predict(test_dataset)\n",
    "logits = pred_out.predictions\n",
    "\n",
    "import numpy as np, torch\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Convert logits\n",
    "if logits.ndim == 2 and logits.shape[1] == 2:\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "else:\n",
    "    probs = torch.sigmoid(torch.tensor(logits).reshape(-1)).numpy()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "\n",
    "y_true = np.array(test_dataset.labels)\n",
    "\n",
    "auc = roc_auc_score(y_true, probs)\n",
    "acc = accuracy_score(y_true, preds)\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_true, preds, average='binary')\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()\n",
    "sens = tp / (tp + fn)\n",
    "spec = tn / (tn + fp)\n",
    "\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {p:.4f}\")\n",
    "print(f\"Recall/Sensitivity: {r:.4f}\")\n",
    "print(f\"F1: {f1:.4f}\")\n",
    "print(f\"Sensitivity: {sens:.4f}\")\n",
    "print(f\"Specificity: {spec:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80b7f484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anudeep/miniconda3/envs/tf_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_out.predictions.shape = (1500, 2)\n",
      "\n",
      "=== TEST METRICS ===\n",
      "AUC:         0.8156\n",
      "Accuracy:    0.7347\n",
      "Precision:   0.7442\n",
      "Recall:      0.7226\n",
      "F1:          0.7332\n",
      "Sensitivity: 0.7226\n",
      "Specificity: 0.7470\n",
      "Confusion matrix (tn, fp, fn, tp): (np.int64(555), np.int64(188), np.int64(210), np.int64(547))\n"
     ]
    }
   ],
   "source": [
    "# === PREDICT ON TEST SET & COMPUTE METRICS ===\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# Run predictions (fast if already computed)\n",
    "pred_out = trainer.predict(test_dataset)\n",
    "logits = pred_out.predictions\n",
    "print(\"pred_out.predictions.shape =\", getattr(logits, \"shape\", None))\n",
    "\n",
    "# Convert logits -> probabilities for positive class\n",
    "if getattr(logits, \"ndim\", None) == 2 and logits.shape[1] == 2:\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "else:\n",
    "    probs = torch.sigmoid(torch.tensor(np.asarray(logits).reshape(-1))).numpy()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "\n",
    "# Ground-truth labels from SimpleDS (or fallback to pandas test DataFrame)\n",
    "if hasattr(test_dataset, \"labels\"):\n",
    "    y_true = np.asarray(test_dataset.labels).reshape(-1)\n",
    "else:\n",
    "    y_true = np.asarray(test['glaucoma']).reshape(-1)\n",
    "\n",
    "# Basic metrics\n",
    "auc = roc_auc_score(y_true, probs) if len(np.unique(y_true)) > 1 else float('nan')\n",
    "acc = accuracy_score(y_true, preds)\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_true, preds, average='binary', zero_division=0)\n",
    "\n",
    "# Sensitivity (recall for positive class) and Specificity (recall for negative class)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "print(\"\\n=== TEST METRICS ===\")\n",
    "print(f\"AUC:         {auc:.4f}\")\n",
    "print(f\"Accuracy:    {acc:.4f}\")\n",
    "print(f\"Precision:   {p:.4f}\")\n",
    "print(f\"Recall:      {r:.4f}\")\n",
    "print(f\"F1:          {f1:.4f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(\"Confusion matrix (tn, fp, fn, tp):\", (tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b12db42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AUC by Race Group ===\n",
      "Asian: AUC = 0.8267  (n=123)\n",
      "Black: AUC = 0.7766  (n=224)\n",
      "White: AUC = 0.8236  (n=1153)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "df_test = test.reset_index(drop=True).copy()\n",
    "df_test['prob_pos'] = probs\n",
    "df_test['pred'] = preds\n",
    "\n",
    "# Normalize to lowercase so comparisons always work\n",
    "df_test['race_norm'] = df_test['race'].astype(str).str.strip().str.lower()\n",
    "\n",
    "groups = [\"asian\", \"black\", \"white\"]   # use lowercase\n",
    "\n",
    "print(\"\\n=== AUC by Race Group ===\")\n",
    "for g in groups:\n",
    "    mask = df_test['race_norm'] == g\n",
    "    n = mask.sum()\n",
    "    if n == 0:\n",
    "        print(f\"{g}: no samples in test set (n=0)\")\n",
    "        continue\n",
    "\n",
    "    y_true_g = df_test.loc[mask, 'glaucoma'].astype(int).values\n",
    "    probs_g = df_test.loc[mask, 'prob_pos'].values\n",
    "    \n",
    "    auc_g = roc_auc_score(y_true_g, probs_g) if len(set(y_true_g)) > 1 else float('nan')\n",
    "    print(f\"{g.capitalize()}: AUC = {auc_g:.4f}  (n={n})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b148dbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model & tokenizer to: /Users/anudeep/Documents/glaucoma_detection/models/transformer_tiny-biobert\n",
      "Saved test predictions to: /Users/anudeep/Documents/glaucoma_detection/models/transformer_tiny-biobert/test_predictions_transformer.csv\n",
      "Saved metrics: /Users/anudeep/Documents/glaucoma_detection/models/transformer_tiny-biobert/transformer_metrics.json\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "# OUT_DIR_MODEL should already be set (e.g. /.../models/transformer_tiny-biobert)\n",
    "os.makedirs(OUT_DIR_MODEL, exist_ok=True)\n",
    "\n",
    "# 1) Save model & tokenizer\n",
    "trainer.save_model(OUT_DIR_MODEL)            # saves model + config\n",
    "tokenizer.save_pretrained(OUT_DIR_MODEL)    # saves tokenizer files\n",
    "print(\"Saved model & tokenizer to:\", OUT_DIR_MODEL)\n",
    "\n",
    "# 2) Save test predictions dataframe (df_test must have prob_pos, pred, glaucoma, race)\n",
    "preds_csv = os.path.join(OUT_DIR_MODEL, \"test_predictions_transformer.csv\")\n",
    "df_test.to_csv(preds_csv, index=False)\n",
    "print(\"Saved test predictions to:\", preds_csv)\n",
    "\n",
    "# 3) Save metrics JSON (fill in with your computed values)\n",
    "metrics = {\n",
    "    \"auc\": float(auc),\n",
    "    \"accuracy\": float(acc),\n",
    "    \"precision\": float(p),\n",
    "    \"recall\": float(r),\n",
    "    \"f1\": float(f1),\n",
    "    \"sensitivity\": float(sensitivity),\n",
    "    \"specificity\": float(specificity),\n",
    "    \"auc_asian\": float(df_test.loc[df_test['race_norm']=='asian','prob_pos'].pipe(lambda p: float('nan') if p.size==0 else roc_auc_score(df_test.loc[df_test['race_norm']=='asian','glaucoma'], p))),\n",
    "    \"auc_black\": float(df_test.loc[df_test['race_norm']=='black','prob_pos'].pipe(lambda p: float('nan') if p.size==0 else roc_auc_score(df_test.loc[df_test['race_norm']=='black','glaucoma'], p))),\n",
    "    \"auc_white\": float(df_test.loc[df_test['race_norm']=='white','prob_pos'].pipe(lambda p: float('nan') if p.size==0 else roc_auc_score(df_test.loc[df_test['race_norm']=='white','glaucoma'], p))),\n",
    "}\n",
    "with open(os.path.join(OUT_DIR_MODEL, \"transformer_metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"Saved metrics:\", os.path.join(OUT_DIR_MODEL,\"transformer_metrics.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79cc0a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: The patient has no issues. Vision normal. No glaucoma symptoms.\n",
      "Prediction: No Glaucoma\n",
      "Probability: 0.0094\n",
      "\n",
      "Input: Optic disc cupping noted with elevated IOP. Possible glaucoma.\n",
      "Prediction: Glaucoma\n",
      "Probability: 0.8778\n",
      "\n",
      "Input: Patient complains of stomach pain only. No eye complaints.\n",
      "Prediction: No Glaucoma\n",
      "Probability: 0.0113\n",
      "\n",
      "Input: Severe optic nerve damage and high intraocular pressure.\n",
      "Prediction: Glaucoma\n",
      "Probability: 0.7794\n"
     ]
    }
   ],
   "source": [
    "# --- Minimal Inference Program for Glaucoma Detection ---\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def predict_text(text):\n",
    "    \"\"\"\n",
    "    Input: any natural language text (string)\n",
    "    Output: predicted_label (0/1), probability_glaucoma (0.0-1.0)\n",
    "    Works with your trained Transformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # set model to eval mode\n",
    "\n",
    "    # tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # move tensors to the right device\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # 2-class softmax\n",
    "    if logits.shape[-1] == 2:\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        pred = int(np.argmax(probs))\n",
    "        prob_glaucoma = float(probs[1])\n",
    "    else:\n",
    "        # 1-logit sigmoid case\n",
    "        prob_glaucoma = float(torch.sigmoid(logits).cpu().numpy().reshape(-1)[0])\n",
    "        pred = 1 if prob_glaucoma > 0.5 else 0\n",
    "\n",
    "    return pred, prob_glaucoma\n",
    "\n",
    "\n",
    "# --- Try some examples ---\n",
    "examples = [\n",
    "    \"The patient has no issues. Vision normal. No glaucoma symptoms.\",   # should be 0\n",
    "    \"Optic disc cupping noted with elevated IOP. Possible glaucoma.\",     # should be 1\n",
    "    \"Patient complains of stomach pain only. No eye complaints.\",         # should be 0\n",
    "    \"Severe optic nerve damage and high intraocular pressure.\",           # should be 1\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    pred, prob = predict_text(text)\n",
    "    print(f\"\\nInput: {text}\")\n",
    "    print(f\"Prediction: {'Glaucoma' if pred==1 else 'No Glaucoma'}\")\n",
    "    print(f\"Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d886977",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
